import os
import glob
import json
import random
import re
from typing import Dict, List, Tuple, Optional

import torch
from torch.utils import data
import numpy as np
from collections import defaultdict

from .shapenet import category_ids as base_category_ids


# 4ID dataset mappings (based on four_id_vocab.json and four_id_category_ids.json)
four_id_major_categories = {
    'cabinet': 0, 'desk': 1, 'dishwasher': 2, 'refrigerator': 3, 
    'suitcase': 4, 'table': 5, 'trash': 6
}  # 7 major categories

four_id_part_categories = {
    'body': 0, 'door': 1, 'drawer': 2, 'handle': 3, 'lid': 4, 
    'other': 5, 'shelf': 6, 'wheel': 7
}  # 8 part categories

# This dictionary will be populated through files generated by four_id_mapping.json
four_id_category_ids = {
    # Format: 'category-part_type-object_id-instance_id': category_id
}


def load_four_id_mappings(mapping_file: str) -> Dict[str, Dict]:
    """Load all mapping information from 4ID mapping file"""
    mappings = {}
    if os.path.exists(mapping_file):
        with open(mapping_file, 'r', encoding='utf-8') as f:
            mappings = json.load(f)
    return mappings


def create_four_id_identifier_mapping(mappings: Dict[str, Dict]) -> Dict[str, int]:
    """Create 4ID identifier mapping: object_id-instance_id -> integer ID"""
    identifier_mapping = {}
    identifier_counter = 0
    
    for synset, info in mappings.items():
        object_id = info['object_id'] 
        instance_id = info['instance_id']
        identifier = f"{object_id}-{instance_id}"
        
        if identifier not in identifier_mapping:
            identifier_mapping[identifier] = identifier_counter
            identifier_counter += 1
    
    return identifier_mapping


def parse_four_id_synset(synset: str) -> Tuple[str, str, str, int]:
    """
    Parse 4ID synset name and extract four types of IDs
    Example: 'cabinet-door-101584-0' -> ('cabinet', 'door', '101584', 0)
    """
    parts = synset.split('-')
    if len(parts) >= 4:
        category = parts[0]
        part_type = parts[1] 
        # Handle possible compound part_type (e.g., slider-drawer)
        if len(parts) > 4:
            # Last part is instance_id, second to last is object_id
            instance_id = int(parts[-1])
            object_id = parts[-2]
            part_type = '-'.join(parts[1:-2])  # Middle part is part_type
        else:
            object_id = parts[2]
            instance_id = int(parts[3])
            
        return category, part_type, object_id, instance_id
    else:
        raise ValueError(f"Unable to parse 4ID synset name: {synset}")


class ArticulatedDataset(data.Dataset):
    """
    Articulated dataset loader supporting 4ID, compatible with 3ID model architecture
    Four types of IDs are:
    1. Major category: 'cabinet', 'desk', etc.
    2. Part type: 'body', 'door', 'drawer', etc.
    3. Object identifier: '101584', '102194', etc.
    4. Instance identifier: 0, 1, 2, 3
    
    For 3ID model compatibility, object_id and instance_id are combined into identifier: "object_id-instance_id"
    """
    
    def __init__(self, dataset_folder, split, categories=None, transform=None, 
                 sampling=True, num_samples=4096, return_surface=True, 
                 surface_sampling=True, pc_size=2048, replica=16,
                 four_id_mapping_file=None):
        
        self.pc_size = pc_size
        self.transform = transform
        self.num_samples = num_samples
        self.sampling = sampling
        self.split = split
        self.dataset_folder = dataset_folder
        self.return_surface = return_surface
        self.surface_sampling = surface_sampling
        self.replica = replica

        # Data paths
        self.point_folder = os.path.join(self.dataset_folder, 'ShapeNetV2_point')
        self.mesh_folder = os.path.join(self.dataset_folder, 'ShapeNetV2_watertight')

        # Load 4ID mappings
        self.four_id_mappings = {}
        self.identifier_to_id = {}
        
        if four_id_mapping_file and os.path.exists(four_id_mapping_file):
            self.four_id_mappings = load_four_id_mappings(four_id_mapping_file)
            self.identifier_to_id = create_four_id_identifier_mapping(self.four_id_mappings)
            print(f"Loaded {len(self.four_id_mappings)} 4ID mappings")
            print(f"Created {len(self.identifier_to_id)} identifier mappings")

        # Get all available categories (synsets)
        if categories is None:
            categories = os.listdir(self.point_folder)
            categories = [c for c in categories if os.path.isdir(os.path.join(self.point_folder, c))]
        
        # Filter to keep only 4ID format categories
        valid_categories = []
        for c in categories:
            try:
                # Try to parse, if successful then it's valid 4ID format
                parse_four_id_synset(c)
                valid_categories.append(c)
            except ValueError:
                continue
        
        categories = sorted(valid_categories)
        print(f"Found {len(categories)} valid 4ID categories")

        # Collect model data
        self.models = []
        self.object_to_models = defaultdict(list)  # Group by object_id
        
        for c_idx, c in enumerate(categories):
            subpath = os.path.join(self.point_folder, c)
            if not os.path.isdir(subpath):
                continue

            split_file = os.path.join(subpath, split + '.lst')
            if not os.path.exists(split_file):
                continue
                
            with open(split_file, 'r') as f:
                models_c = [ln.strip() for ln in f.read().split('\n') if ln.strip()]

            # Verify file existence
            valid = []
            for m in models_c:
                stem = m.replace('.npz', '')
                p_npz = os.path.join(self.point_folder, c, f'{stem}.npz')
                p_npy = os.path.join(self.point_folder, c, f'{stem}.npy')
                s_npz = os.path.join(self.mesh_folder, c, '4_pointcloud', f'{stem}.npz')
                
                if os.path.exists(p_npz) and os.path.exists(p_npy) and os.path.exists(s_npz):
                    model_info = {
                        'category': c,
                        'model': stem,
                        'synset': c  # synset is category
                    }
                    
                    # Parse 4ID
                    try:
                        category, part_type, object_id, instance_id = parse_four_id_synset(c)
                        model_info.update({
                            'major_category': category,
                            'part_category': part_type,
                            'object_id': object_id,
                            'instance_id': instance_id,
                            'identifier': f"{object_id}-{instance_id}"  # Combined identifier
                        })
                        valid.append(model_info)
                        
                        # Group by object_id
                        self.object_to_models[object_id].append(model_info)
                        
                    except ValueError as e:
                        print(f"Skip model {c}/{stem}: {e}")
                        continue
                        
            self.models.extend(valid)

        print(f"Total loaded {len(self.models)} models")
        print(f"Covering {len(self.object_to_models)} different objects")

    def get_four_ids(self, idx):
        """Get three types of IDs for the specified index model (compatible with 3ID model format)"""
        model = self.models[idx % len(self.models)]
        major_category_id = four_id_major_categories.get(model['major_category'], 0)
        part_category_id = four_id_part_categories.get(model['part_category'], 0)
        identifier = model['identifier']  # "object_id-instance_id" format
        return major_category_id, part_category_id, identifier

    def get_models_by_object_id(self, object_id: str) -> List[Dict]:
        """Get all part models for the specified object_id"""
        return self.object_to_models.get(object_id, [])

    def get_random_model_with_same_object(self, object_id: str) -> Optional[Dict]:
        """Randomly get another part model of the same object"""
        models = self.get_models_by_object_id(object_id)
        if len(models) > 1:
            return random.choice(models)
        return None

    def __getitem__(self, idx):
        idx = idx % len(self.models)
        model = self.models[idx]
        
        category = model['category']
        model_name = model['model']
        
        # Load point cloud data
        point_path = os.path.join(self.point_folder, category, model_name + '.npz')
        try:
            with np.load(point_path) as data:
                vol_points = data['vol_points']
                vol_label = data['vol_label']
                near_points = data['near_points']
                near_label = data['near_label']
        except Exception as e:
            print(f"Failed to load point cloud data: {e}")
            print(f"Path: {point_path}")
            # Return default data
            return self.__getitem__((idx + 1) % len(self.models))

        # Load scale information
        with open(point_path.replace('.npz', '.npy'), 'rb') as f:
            scale = np.load(f).item()

        # Load surface point cloud
        if self.return_surface:
            pc_path = os.path.join(self.mesh_folder, category, '4_pointcloud', model_name + '.npz')
            with np.load(pc_path) as data:
                surface = data['points'].astype(np.float32)
                
            if self.surface_sampling:
                ind = np.random.default_rng().choice(surface.shape[0], self.pc_size, replace=False)
                surface = surface[ind]
            surface = torch.from_numpy(surface)

        # Sampling
        if self.sampling:
            ind = np.random.default_rng().choice(vol_points.shape[0], self.num_samples, replace=False)
            vol_points = vol_points[ind]
            vol_label = vol_label[ind]

            ind = np.random.default_rng().choice(near_points.shape[0], self.num_samples, replace=False)
            near_points = near_points[ind]
            near_label = near_label[ind]

        # Convert to tensor
        vol_points = torch.from_numpy(vol_points)
        vol_label = torch.from_numpy(vol_label).float()

        if self.split == 'train':
            near_points = torch.from_numpy(near_points)
            near_label = torch.from_numpy(near_label).float()
            points = torch.cat([vol_points, near_points], dim=0)
            labels = torch.cat([vol_label, near_label], dim=0)
        else:
            points = vol_points
            labels = vol_label

        # Apply transformations
        if self.transform:
            surface, points = self.transform(surface, points)

        # Get three types of IDs (compatible with 3ID model)
        major_category_id, part_category_id, identifier = self.get_four_ids(idx)
        
        # Get traditional category_id (for compatibility)
        category_id = four_id_category_ids.get(category, 0)

        if self.return_surface:
            return (points, labels, surface, category_id, 
                   major_category_id, part_category_id, identifier)
        else:
            return (points, labels, category_id, 
                   major_category_id, part_category_id, identifier)

    def __len__(self):
        if self.split != 'train':
            return len(self.models)
        else:
            return len(self.models) * self.replica


def build_articulated_dataset(split, args):
    """Build 4ID articulated dataset"""
    transform = None
    if split == 'train' and hasattr(args, 'use_transform') and args.use_transform:
        from .datasets import AxisScaling
        transform = AxisScaling((0.75, 1.25), True)
    
    four_id_mapping_file = getattr(args, 'four_id_mapping_file', None)
    
    return ArticulatedDataset(
        dataset_folder=args.data_path,
        split=split,
        transform=transform,
        sampling=(split == 'train'),
        num_samples=1024,
        return_surface=True,
        surface_sampling=True,
        pc_size=getattr(args, 'point_cloud_size', 2048),
        four_id_mapping_file=four_id_mapping_file
    )


if __name__ == '__main__':
    # Test code
    class Args:
        data_path = '/home/zhaochaoyang/yuantingyu/3DShape2vecset/data_articulated_full_training_v2'
        point_cloud_size = 2048
        four_id_mapping_file = '/home/zhaochaoyang/yuantingyu/3DShape2vecset/data_articulated_full_training_v2/four_id_mapping.json'
    
    args = Args()
    dataset = build_articulated_dataset('train', args)
    print(f"Dataset size: {len(dataset)}")
    
    if len(dataset) > 0:
        sample = dataset[0]
        print(f"Sample data structure: {[x.shape if hasattr(x, 'shape') else type(x) for x in sample]}")